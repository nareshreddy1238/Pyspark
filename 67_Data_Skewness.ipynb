{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a49b38ba-fad0-4b89-8e13-aa1260285ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10127"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('com1').getOrCreate()\n",
    "df = spark.read.csv(\"BankChurners.csv\",header=True,inferSchema=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7827c46-e949-4aab-ae63-67d2f70de8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48d09ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d4b2c01-3316-40c2-8cd5-6a7c94a1a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0| 2531|\n",
      "|          1| 2532|\n",
      "|          2| 2532|\n",
      "|          3| 2532|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df2 = df1.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ba7ce",
   "metadata": {},
   "source": [
    "### repartitioned By using Column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8aed5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df1.repartition(8, 'Card_category')*+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "000f92a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99afa659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0| 9436|\n",
      "|          2|  116|\n",
      "|          6|  575|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df5 = df4.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "994a9c99-1bde-41c9-bb36-fdf141bc7514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name| value|\n",
      "+----+------+\n",
      "|   x|    66|\n",
      "|   x|    33|\n",
      "|   x|    89|\n",
      "|   x|    77|\n",
      "|   x|    88|\n",
      "|   z|    93|\n",
      "|   x|    44|\n",
      "|   x|    22|\n",
      "|   c|     3|\n",
      "|   y|    92|\n",
      "|   x|    88|\n",
      "|   x|    90|\n",
      "|   x|    44|\n",
      "|   x|    22|\n",
      "|   b|     2|\n",
      "|   x|    11|\n",
      "|   y|    91|\n",
      "|   x|    90|\n",
      "|   d|     4|\n",
      "|   x|    44|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           3|    7|\n",
      "|           0|    6|\n",
      "|           2|    6|\n",
      "|           1|    6|\n",
      "|           4|    6|\n",
      "|           5|    6|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### 2.Another Example\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('com1').getOrCreate()\n",
    "d1 = spark.read.csv(\"data_skew_file.csv\",header=True)\n",
    "d= d1.repartition(6)\n",
    "d.show()\n",
    "d.rdd.getNumPartitions()\n",
    "\n",
    "from pyspark.sql.functions import spark_partition_id, asc, desc\n",
    "\n",
    "d = d.withColumn(\"partition_id\",spark_partition_id())\n",
    "df2 = d.withColumn(\"partition_id\", spark_partition_id())\\\n",
    "      .groupBy(\"partition_id\")\\\n",
    "      .count()\\\n",
    "      .orderBy(desc(\"count\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ee40177-4030-44b5-9514-c3d39695f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|_1 |_2 |\n",
      "+---+---+\n",
      "|x  |bc |\n",
      "|x  |ce |\n",
      "|x  |ab |\n",
      "|x  |ef |\n",
      "|x  |gh |\n",
      "|y  |hk |\n",
      "|z  |jk |\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|_1 |_2 |\n",
      "+---+---+\n",
      "|x  |gkl|\n",
      "|y  |nmb|\n",
      "|z  |qwe|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|_1 |_2 |\n",
      "+---+---+\n",
      "|x_5|bc |\n",
      "|x_9|ce |\n",
      "|x_5|ab |\n",
      "|x_6|ef |\n",
      "|x_4|gh |\n",
      "|y_9|hk |\n",
      "|z_6|jk |\n",
      "+---+---+\n",
      "\n",
      "+---+---+-----------+\n",
      "|_1 |_2 |explodedCol|\n",
      "+---+---+-----------+\n",
      "|x  |gkl|0          |\n",
      "|x  |gkl|1          |\n",
      "|x  |gkl|2          |\n",
      "|x  |gkl|3          |\n",
      "|x  |gkl|4          |\n",
      "|x  |gkl|5          |\n",
      "|x  |gkl|6          |\n",
      "|x  |gkl|7          |\n",
      "|x  |gkl|8          |\n",
      "|x  |gkl|9          |\n",
      "|x  |gkl|10         |\n",
      "|y  |nmb|0          |\n",
      "|y  |nmb|1          |\n",
      "|y  |nmb|2          |\n",
      "|y  |nmb|3          |\n",
      "|y  |nmb|4          |\n",
      "|y  |nmb|5          |\n",
      "|y  |nmb|6          |\n",
      "|y  |nmb|7          |\n",
      "|y  |nmb|8          |\n",
      "|y  |nmb|9          |\n",
      "|y  |nmb|10         |\n",
      "|z  |qwe|0          |\n",
      "|z  |qwe|1          |\n",
      "|z  |qwe|2          |\n",
      "|z  |qwe|3          |\n",
      "|z  |qwe|4          |\n",
      "|z  |qwe|5          |\n",
      "|z  |qwe|6          |\n",
      "|z  |qwe|7          |\n",
      "|z  |qwe|8          |\n",
      "|z  |qwe|9          |\n",
      "|z  |qwe|10         |\n",
      "+---+---+-----------+\n",
      "\n",
      "+---+---+---+---+-----------+\n",
      "|_1 |_2 |_1 |_2 |explodedCol|\n",
      "+---+---+---+---+-----------+\n",
      "|x_4|gh |x  |gkl|4          |\n",
      "|x_5|bc |x  |gkl|5          |\n",
      "|x_5|ab |x  |gkl|5          |\n",
      "|x_6|ef |x  |gkl|6          |\n",
      "|x_9|ce |x  |gkl|9          |\n",
      "|y_9|hk |y  |nmb|9          |\n",
      "|z_6|jk |z  |qwe|6          |\n",
      "+---+---+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, explode, floor, lit, rand, array\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RemoveDataSkewness\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "data1 = [\n",
    "    (\"x\", \"bc\"),\n",
    "    (\"x\", \"ce\"),\n",
    "    (\"x\", \"ab\"),\n",
    "    (\"x\", \"ef\"),\n",
    "    (\"x\", \"gh\"),\n",
    "    (\"y\", \"hk\"),\n",
    "    (\"z\", \"jk\")\n",
    "]\n",
    "columns1 = [\"_1\", \"_2\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df1.show(10, False)\n",
    "\n",
    "data2 = [\n",
    "    (\"x\", \"gkl\"),\n",
    "    (\"y\", \"nmb\"),\n",
    "    (\"z\", \"qwe\")\n",
    "]\n",
    "columns2 = [\"_1\", \"_2\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "df2.show(10, False)\n",
    "\n",
    "# Method to eliminate data skewness\n",
    "def eliminate_data_skew(left_table, left_col, right_table):\n",
    "    df1 = left_table.withColumn(\n",
    "        left_col,\n",
    "        concat(left_table[left_col], lit(\"_\"), floor(rand() * 10))\n",
    "    )\n",
    "    df2 = right_table.withColumn(\n",
    "        \"explodedCol\",\n",
    "        explode(array([lit(i) for i in range(11)]))\n",
    "    )\n",
    "    return df1, df2\n",
    "\n",
    "df3, df4 = eliminate_data_skew(df1, \"_1\", df2)\n",
    "\n",
    "df3.show(100, False)\n",
    "df4.show(100, False)\n",
    "\n",
    "# Join after eliminating data skewness\n",
    "result = df3.join(\n",
    "    df4,\n",
    "    df3[\"_1\"] == concat(df4[\"_1\"], lit(\"_\"), df4[\"explodedCol\"])\n",
    ")\n",
    "\n",
    "result.show(100, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762229e-bcd1-4205-bd1c-5f177d27b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
