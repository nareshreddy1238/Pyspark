{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457d2b90",
   "metadata": {},
   "source": [
    "### Execution Plan: explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f066d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('app1').getOrCreate()\n",
    "df = spark.read.csv(\"customer.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679eb638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Customer_No: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Customer_No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af216a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [Customer_No#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Customer_No:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Customer_No').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e0b6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+-------------------+----------------+-------+\n",
      "|Customer_No|    Card_type|     Date|           Category|Transaction Type| Amount|\n",
      "+-----------+-------------+---------+-------------------+----------------+-------+\n",
      "|    1000501|Platinum Card| 1/1/2018|           Shopping|           debit|  11.11|\n",
      "|    1000501|     Checking| 1/2/2018|    Mortgage & Rent|           debit|1247.44|\n",
      "|    1000501|  Silver Card| 1/2/2018|        Restaurants|           debit|  24.22|\n",
      "|    1000501|Platinum Card| 1/3/2018|Credit Card Payment|          credit|2298.09|\n",
      "|    1000501|Platinum Card| 1/4/2018|      Movies & DVDs|           debit|  11.76|\n",
      "|    1000501|  Silver Card| 1/5/2018|        Restaurants|           debit|  25.85|\n",
      "|    1000501|  Silver Card| 1/6/2018|   Home Improvement|           debit|  18.45|\n",
      "|    1000501|     Checking| 1/8/2018|          Utilities|           debit|     45|\n",
      "|    1000501|  Silver Card| 1/8/2018|   Home Improvement|           debit|  15.38|\n",
      "|    1000501|Platinum Card| 1/9/2018|              Music|           debit|  10.69|\n",
      "|    1000501|     Checking|1/10/2018|       Mobile Phone|           debit|  89.46|\n",
      "|    1000501|Platinum Card|1/11/2018|         Gas & Fuel|           debit|  34.87|\n",
      "|    1000501|Platinum Card|1/11/2018|          Groceries|           debit|  43.54|\n",
      "|    1000501|     Checking|1/12/2018|           Paycheck|          credit|   2000|\n",
      "|    1000531|Platinum Card|1/13/2018|          Fast Food|           debit|  32.91|\n",
      "|    1000531|Platinum Card|1/13/2018|           Shopping|           debit|  39.05|\n",
      "|    1000531|  Silver Card|1/15/2018|          Groceries|           debit|  44.19|\n",
      "|    1000531|  Silver Card|1/15/2018|        Restaurants|           debit|  64.11|\n",
      "|    1000531|     Checking|1/16/2018|          Utilities|           debit|     35|\n",
      "|    1000531|     Checking|1/16/2018|          Utilities|           debit|     60|\n",
      "+-----------+-------------+---------+-------------------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d7bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    Card_Type|count|\n",
      "+-------------+-----+\n",
      "|  Silver Card|   28|\n",
      "|Platinum Card|   35|\n",
      "|     Checking|   32|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c05c903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Card_Type#17], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(Card_Type#17, 200), ENSURE_REQUIREMENTS, [plan_id=241]\n",
      "      +- HashAggregate(keys=[Card_Type#17], functions=[partial_count(1)])\n",
      "         +- Project [Card_type#17]\n",
      "            +- Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "               +- FileScan csv [Card_type#17,Amount#21] Batched: false, DataFilters: [isnotnull(Amount#21), (cast(Amount#21 as int) >= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Amount)], ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320c731e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Card_Type#17], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(Card_Type#17, 200), ENSURE_REQUIREMENTS, [plan_id=258]\n",
      "      +- HashAggregate(keys=[Card_Type#17], functions=[partial_count(1)])\n",
      "         +- Project [Card_type#17]\n",
      "            +- Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "               +- FileScan csv [Card_type#17,Amount#21] Batched: false, DataFilters: [isnotnull(Amount#21), (cast(Amount#21 as int) >= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Amount)], ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().explain(\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1981add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['Card_Type], ['Card_Type, count(1) AS count#256L]\n",
      "+- Filter (cast(Amount#21 as int) >= 10)\n",
      "   +- Relation [Customer_No#16,Card_type#17,Date#18,Category#19,Transaction Type#20,Amount#21] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Card_Type: string, count: bigint\n",
      "Aggregate [Card_Type#17], [Card_Type#17, count(1) AS count#256L]\n",
      "+- Filter (cast(Amount#21 as int) >= 10)\n",
      "   +- Relation [Customer_No#16,Card_type#17,Date#18,Category#19,Transaction Type#20,Amount#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [Card_Type#17], [Card_Type#17, count(1) AS count#256L]\n",
      "+- Project [Card_type#17]\n",
      "   +- Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "      +- Relation [Customer_No#16,Card_type#17,Date#18,Category#19,Transaction Type#20,Amount#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Card_Type#17], functions=[count(1)], output=[Card_Type#17, count#256L])\n",
      "   +- Exchange hashpartitioning(Card_Type#17, 200), ENSURE_REQUIREMENTS, [plan_id=275]\n",
      "      +- HashAggregate(keys=[Card_Type#17], functions=[partial_count(1)], output=[Card_Type#17, count#260L])\n",
      "         +- Project [Card_type#17]\n",
      "            +- Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "               +- FileScan csv [Card_type#17,Amount#21] Batched: false, DataFilters: [isnotnull(Amount#21), (cast(Amount#21 as int) >= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Amount)], ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68da0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [Card_Type#17], [Card_Type#17, count(1) AS count#268L], Statistics(sizeInBytes=1596.0 B)\n",
      "+- Project [Card_type#17], Statistics(sizeInBytes=1242.0 B)\n",
      "   +- Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10)), Statistics(sizeInBytes=5.5 KiB)\n",
      "      +- Relation [Customer_No#16,Card_type#17,Date#18,Category#19,Transaction Type#20,Amount#21] csv, Statistics(sizeInBytes=5.5 KiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Card_Type#17], functions=[count(1)], output=[Card_Type#17, count#268L])\n",
      "   +- Exchange hashpartitioning(Card_Type#17, 200), ENSURE_REQUIREMENTS, [plan_id=292]\n",
      "      +- HashAggregate(keys=[Card_Type#17], functions=[partial_count(1)], output=[Card_Type#17, count#272L])\n",
      "         +- Project [Card_type#17]\n",
      "            +- Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "               +- FileScan csv [Card_type#17,Amount#21] Batched: false, DataFilters: [isnotnull(Amount#21), (cast(Amount#21 as int) >= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Amount)], ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().explain(\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b625cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 2 (maxMethodCodeSize:283; maxConstantPoolSize:269(0.41% used); numInnerClasses:2) ==\n",
      "*(1) HashAggregate(keys=[Card_Type#17], functions=[partial_count(1)], output=[Card_Type#17, count#354L])\n",
      "+- *(1) Project [Card_type#17]\n",
      "   +- *(1) Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "      +- FileScan csv [Card_type#17,Amount#21] Batched: false, DataFilters: [isnotnull(Amount#21), (cast(Amount#21 as int) >= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Amount)], ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private agg_FastHashMap_0 agg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> agg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;\n",
      "/* 017 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 019 */\n",
      "/* 020 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 021 */     this.references = references;\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 025 */     partitionIndex = index;\n",
      "/* 026 */     this.inputs = inputs;\n",
      "/* 027 */\n",
      "/* 028 */     inputadapter_input_0 = inputs[0];\n",
      "/* 029 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\n",
      "/* 030 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 031 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 032 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 033 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 034 */\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */   public class agg_FastHashMap_0 {\n",
      "/* 038 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 039 */     private int[] buckets;\n",
      "/* 040 */     private int capacity = 1 << 16;\n",
      "/* 041 */     private double loadFactor = 0.5;\n",
      "/* 042 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 043 */     private int maxSteps = 2;\n",
      "/* 044 */     private int numRows = 0;\n",
      "/* 045 */     private Object emptyVBase;\n",
      "/* 046 */     private long emptyVOff;\n",
      "/* 047 */     private int emptyVLen;\n",
      "/* 048 */     private boolean isBatchFull = false;\n",
      "/* 049 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;\n",
      "/* 050 */\n",
      "/* 051 */     public agg_FastHashMap_0(\n",
      "/* 052 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 053 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 054 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 055 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 056 */\n",
      "/* 057 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 058 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 059 */\n",
      "/* 060 */       emptyVBase = emptyBuffer;\n",
      "/* 061 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 062 */       emptyVLen = emptyBuffer.length;\n",
      "/* 063 */\n",
      "/* 064 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 065 */         1, 32);\n",
      "/* 066 */\n",
      "/* 067 */       buckets = new int[numBuckets];\n",
      "/* 068 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 069 */     }\n",
      "/* 070 */\n",
      "/* 071 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String agg_key_0) {\n",
      "/* 072 */       long h = hash(agg_key_0);\n",
      "/* 073 */       int step = 0;\n",
      "/* 074 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 075 */       while (step < maxSteps) {\n",
      "/* 076 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 077 */         if (buckets[idx] == -1) {\n",
      "/* 078 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 079 */             agg_rowWriter.reset();\n",
      "/* 080 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 081 */             agg_rowWriter.write(0, agg_key_0);\n",
      "/* 082 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 083 */             = agg_rowWriter.getRow();\n",
      "/* 084 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 085 */             long koff = agg_result.getBaseOffset();\n",
      "/* 086 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 087 */\n",
      "/* 088 */             UnsafeRow vRow\n",
      "/* 089 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 090 */             if (vRow == null) {\n",
      "/* 091 */               isBatchFull = true;\n",
      "/* 092 */             } else {\n",
      "/* 093 */               buckets[idx] = numRows++;\n",
      "/* 094 */             }\n",
      "/* 095 */             return vRow;\n",
      "/* 096 */           } else {\n",
      "/* 097 */             // No more space\n",
      "/* 098 */             return null;\n",
      "/* 099 */           }\n",
      "/* 100 */         } else if (equals(idx, agg_key_0)) {\n",
      "/* 101 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 102 */         }\n",
      "/* 103 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 104 */         step++;\n",
      "/* 105 */       }\n",
      "/* 106 */       // Didn't find it\n",
      "/* 107 */       return null;\n",
      "/* 108 */     }\n",
      "/* 109 */\n",
      "/* 110 */     private boolean equals(int idx, UTF8String agg_key_0) {\n",
      "/* 111 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 112 */       return (row.getUTF8String(0).equals(agg_key_0));\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     private long hash(UTF8String agg_key_0) {\n",
      "/* 116 */       long agg_hash_0 = 0;\n",
      "/* 117 */\n",
      "/* 118 */       int agg_result_0 = 0;\n",
      "/* 119 */       byte[] agg_bytes_0 = agg_key_0.getBytes();\n",
      "/* 120 */       for (int i = 0; i < agg_bytes_0.length; i++) {\n",
      "/* 121 */         int agg_hash_1 = agg_bytes_0[i];\n",
      "/* 122 */         agg_result_0 = (agg_result_0 ^ (0x9e3779b9)) + agg_hash_1 + (agg_result_0 << 6) + (agg_result_0 >>> 2);\n",
      "/* 123 */       }\n",
      "/* 124 */\n",
      "/* 125 */       agg_hash_0 = (agg_hash_0 ^ (0x9e3779b9)) + agg_result_0 + (agg_hash_0 << 6) + (agg_hash_0 >>> 2);\n",
      "/* 126 */\n",
      "/* 127 */       return agg_hash_0;\n",
      "/* 128 */     }\n",
      "/* 129 */\n",
      "/* 130 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 131 */       return batch.rowIterator();\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public void close() {\n",
      "/* 135 */       batch.close();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */   }\n",
      "/* 139 */\n",
      "/* 140 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)\n",
      "/* 141 */   throws java.io.IOException {\n",
      "/* 142 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* numOutputRows */).add(1);\n",
      "/* 143 */\n",
      "/* 144 */     boolean agg_isNull_7 = agg_keyTerm_0.isNullAt(0);\n",
      "/* 145 */     UTF8String agg_value_8 = agg_isNull_7 ?\n",
      "/* 146 */     null : (agg_keyTerm_0.getUTF8String(0));\n",
      "/* 147 */     long agg_value_9 = agg_bufferTerm_0.getLong(0);\n",
      "/* 148 */\n",
      "/* 149 */     filter_mutableStateArray_0[4].reset();\n",
      "/* 150 */\n",
      "/* 151 */     filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 152 */\n",
      "/* 153 */     if (agg_isNull_7) {\n",
      "/* 154 */       filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       filter_mutableStateArray_0[4].write(0, agg_value_8);\n",
      "/* 157 */     }\n",
      "/* 158 */\n",
      "/* 159 */     filter_mutableStateArray_0[4].write(1, agg_value_9);\n",
      "/* 160 */     append((filter_mutableStateArray_0[4].getRow()));\n",
      "/* 161 */\n",
      "/* 162 */   }\n",
      "/* 163 */\n",
      "/* 164 */   private void agg_doConsume_0(UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 165 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;\n",
      "/* 166 */     UnsafeRow agg_fastAggBuffer_0 = null;\n",
      "/* 167 */\n",
      "/* 168 */     if (!agg_exprIsNull_0_0) {\n",
      "/* 169 */       agg_fastAggBuffer_0 = agg_fastHashMap_0.findOrInsert(\n",
      "/* 170 */         agg_expr_0_0);\n",
      "/* 171 */     }\n",
      "/* 172 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 173 */     if (agg_fastAggBuffer_0 == null) {\n",
      "/* 174 */       // generate grouping key\n",
      "/* 175 */       filter_mutableStateArray_0[3].reset();\n",
      "/* 176 */\n",
      "/* 177 */       filter_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 178 */\n",
      "/* 179 */       if (agg_exprIsNull_0_0) {\n",
      "/* 180 */         filter_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 181 */       } else {\n",
      "/* 182 */         filter_mutableStateArray_0[3].write(0, agg_expr_0_0);\n",
      "/* 183 */       }\n",
      "/* 184 */       int agg_unsafeRowKeyHash_0 = (filter_mutableStateArray_0[3].getRow()).hashCode();\n",
      "/* 185 */       if (true) {\n",
      "/* 186 */         // try to get the buffer from hash map\n",
      "/* 187 */         agg_unsafeRowAggBuffer_0 =\n",
      "/* 188 */         agg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[3].getRow()), agg_unsafeRowKeyHash_0);\n",
      "/* 189 */       }\n",
      "/* 190 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 191 */       // aggregation after processing all input rows.\n",
      "/* 192 */       if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 193 */         if (agg_sorter_0 == null) {\n",
      "/* 194 */           agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 195 */         } else {\n",
      "/* 196 */           agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 197 */         }\n",
      "/* 198 */\n",
      "/* 199 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 200 */         // try to allocate buffer again.\n",
      "/* 201 */         agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 202 */           (filter_mutableStateArray_0[3].getRow()), agg_unsafeRowKeyHash_0);\n",
      "/* 203 */         if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 204 */           // failed to allocate the first page\n",
      "/* 205 */           throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 206 */         }\n",
      "/* 207 */       }\n",
      "/* 208 */\n",
      "/* 209 */     }\n",
      "/* 210 */\n",
      "/* 211 */     // Updates the proper row buffer\n",
      "/* 212 */     if (agg_fastAggBuffer_0 != null) {\n",
      "/* 213 */       agg_unsafeRowAggBuffer_0 = agg_fastAggBuffer_0;\n",
      "/* 214 */     }\n",
      "/* 215 */\n",
      "/* 216 */     // common sub-expressions\n",
      "/* 217 */\n",
      "/* 218 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 219 */\n",
      "/* 220 */     long agg_value_6 = agg_unsafeRowAggBuffer_0.getLong(0);\n",
      "/* 221 */\n",
      "/* 222 */     long agg_value_5 = -1L;\n",
      "/* 223 */\n",
      "/* 224 */     agg_value_5 = agg_value_6 + 1L;\n",
      "/* 225 */\n",
      "/* 226 */     agg_unsafeRowAggBuffer_0.setLong(0, agg_value_5);\n",
      "/* 227 */\n",
      "/* 228 */   }\n",
      "/* 229 */\n",
      "/* 230 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 231 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 232 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 233 */\n",
      "/* 234 */       do {\n",
      "/* 235 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 236 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 237 */         null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 238 */\n",
      "/* 239 */         boolean filter_value_2 = !inputadapter_isNull_1;\n",
      "/* 240 */         if (!filter_value_2) continue;\n",
      "/* 241 */\n",
      "/* 242 */         boolean filter_isNull_2 = true;\n",
      "/* 243 */         boolean filter_value_3 = false;\n",
      "/* 244 */         boolean filter_isNull_3 = inputadapter_isNull_1;\n",
      "/* 245 */         int filter_value_4 = -1;\n",
      "/* 246 */         if (!inputadapter_isNull_1) {\n",
      "/* 247 */           UTF8String.IntWrapper filter_intWrapper_0 = new UTF8String.IntWrapper();\n",
      "/* 248 */           if (inputadapter_value_1.toInt(filter_intWrapper_0)) {\n",
      "/* 249 */             filter_value_4 = filter_intWrapper_0.value;\n",
      "/* 250 */           } else {\n",
      "/* 251 */             filter_isNull_3 = true;\n",
      "/* 252 */           }\n",
      "/* 253 */           filter_intWrapper_0 = null;\n",
      "/* 254 */         }\n",
      "/* 255 */         if (!filter_isNull_3) {\n",
      "/* 256 */           filter_isNull_2 = false; // resultCode could change nullability.\n",
      "/* 257 */           filter_value_3 = filter_value_4 >= 10;\n",
      "/* 258 */\n",
      "/* 259 */         }\n",
      "/* 260 */         if (filter_isNull_2 || !filter_value_3) continue;\n",
      "/* 261 */\n",
      "/* 262 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 263 */\n",
      "/* 264 */         // common sub-expressions\n",
      "/* 265 */\n",
      "/* 266 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 267 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 268 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 269 */\n",
      "/* 270 */         agg_doConsume_0(inputadapter_value_0, inputadapter_isNull_0);\n",
      "/* 271 */\n",
      "/* 272 */       } while(false);\n",
      "/* 273 */       // shouldStop check is eliminated\n",
      "/* 274 */     }\n",
      "/* 275 */\n",
      "/* 276 */     agg_fastHashMapIter_0 = agg_fastHashMap_0.rowIterator();\n",
      "/* 277 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numTasksFallBacked */));\n",
      "/* 278 */\n",
      "/* 279 */   }\n",
      "/* 280 */\n",
      "/* 281 */   protected void processNext() throws java.io.IOException {\n",
      "/* 282 */     if (!agg_initAgg_0) {\n",
      "/* 283 */       agg_initAgg_0 = true;\n",
      "/* 284 */       agg_fastHashMap_0 = new agg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().taskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 285 */\n",
      "/* 286 */       ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskContext().addTaskCompletionListener(\n",
      "/* 287 */         new org.apache.spark.util.TaskCompletionListener() {\n",
      "/* 288 */           @Override\n",
      "/* 289 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {\n",
      "/* 290 */             agg_fastHashMap_0.close();\n",
      "/* 291 */           }\n",
      "/* 292 */         });\n",
      "/* 293 */\n",
      "/* 294 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 295 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 296 */       agg_doAggregateWithKeys_0();\n",
      "/* 297 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 298 */     }\n",
      "/* 299 */     // output the result\n",
      "/* 300 */\n",
      "/* 301 */     while ( agg_fastHashMapIter_0.next()) {\n",
      "/* 302 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_fastHashMapIter_0.getKey();\n",
      "/* 303 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_fastHashMapIter_0.getValue();\n",
      "/* 304 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);\n",
      "/* 305 */\n",
      "/* 306 */       if (shouldStop()) return;\n",
      "/* 307 */     }\n",
      "/* 308 */     agg_fastHashMap_0.close();\n",
      "/* 309 */\n",
      "/* 310 */     while ( agg_mapIter_0.next()) {\n",
      "/* 311 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();\n",
      "/* 312 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();\n",
      "/* 313 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);\n",
      "/* 314 */       if (shouldStop()) return;\n",
      "/* 315 */     }\n",
      "/* 316 */     agg_mapIter_0.close();\n",
      "/* 317 */     if (agg_sorter_0 == null) {\n",
      "/* 318 */       agg_hashMap_0.free();\n",
      "/* 319 */     }\n",
      "/* 320 */   }\n",
      "/* 321 */\n",
      "/* 322 */ }\n",
      "\n",
      "== Subtree 2 / 2 (maxMethodCodeSize:187; maxConstantPoolSize:211(0.32% used); numInnerClasses:0) ==\n",
      "*(2) HashAggregate(keys=[Card_Type#17], functions=[count(1)], output=[Card_Type#17, count#350L])\n",
      "+- Exchange hashpartitioning(Card_Type#17, 200), ENSURE_REQUIREMENTS, [plan_id=419]\n",
      "   +- *(1) HashAggregate(keys=[Card_Type#17], functions=[partial_count(1)], output=[Card_Type#17, count#354L])\n",
      "      +- *(1) Project [Card_type#17]\n",
      "         +- *(1) Filter (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "            +- FileScan csv [Card_type#17,Amount#21] Batched: false, DataFilters: [isnotnull(Amount#21), (cast(Amount#21 as int) >= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nares/PySpark_Examples/customer.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Amount)], ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */\n",
      "/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 017 */     this.references = references;\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 021 */     partitionIndex = index;\n",
      "/* 022 */     this.inputs = inputs;\n",
      "/* 023 */\n",
      "/* 024 */     inputadapter_input_0 = inputs[0];\n",
      "/* 025 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 026 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)\n",
      "/* 031 */   throws java.io.IOException {\n",
      "/* 032 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);\n",
      "/* 033 */\n",
      "/* 034 */     boolean agg_isNull_5 = agg_keyTerm_0.isNullAt(0);\n",
      "/* 035 */     UTF8String agg_value_5 = agg_isNull_5 ?\n",
      "/* 036 */     null : (agg_keyTerm_0.getUTF8String(0));\n",
      "/* 037 */     long agg_value_6 = agg_bufferTerm_0.getLong(0);\n",
      "/* 038 */\n",
      "/* 039 */     agg_mutableStateArray_0[1].reset();\n",
      "/* 040 */\n",
      "/* 041 */     agg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 042 */\n",
      "/* 043 */     if (agg_isNull_5) {\n",
      "/* 044 */       agg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       agg_mutableStateArray_0[1].write(0, agg_value_5);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     agg_mutableStateArray_0[1].write(1, agg_value_6);\n",
      "/* 050 */     append((agg_mutableStateArray_0[1].getRow()));\n",
      "/* 051 */\n",
      "/* 052 */   }\n",
      "/* 053 */\n",
      "/* 054 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0, long agg_expr_1_0) throws java.io.IOException {\n",
      "/* 055 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;\n",
      "/* 056 */\n",
      "/* 057 */     // generate grouping key\n",
      "/* 058 */     agg_mutableStateArray_0[0].reset();\n",
      "/* 059 */\n",
      "/* 060 */     agg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 061 */\n",
      "/* 062 */     if (agg_exprIsNull_0_0) {\n",
      "/* 063 */       agg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 064 */     } else {\n",
      "/* 065 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);\n",
      "/* 066 */     }\n",
      "/* 067 */     int agg_unsafeRowKeyHash_0 = (agg_mutableStateArray_0[0].getRow()).hashCode();\n",
      "/* 068 */     if (true) {\n",
      "/* 069 */       // try to get the buffer from hash map\n",
      "/* 070 */       agg_unsafeRowAggBuffer_0 =\n",
      "/* 071 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);\n",
      "/* 072 */     }\n",
      "/* 073 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 074 */     // aggregation after processing all input rows.\n",
      "/* 075 */     if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 076 */       if (agg_sorter_0 == null) {\n",
      "/* 077 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 078 */       } else {\n",
      "/* 079 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 080 */       }\n",
      "/* 081 */\n",
      "/* 082 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 083 */       // try to allocate buffer again.\n",
      "/* 084 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 085 */         (agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);\n",
      "/* 086 */       if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 087 */         // failed to allocate the first page\n",
      "/* 088 */         throw new org.apache.spark.memory.SparkOutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 089 */       }\n",
      "/* 090 */     }\n",
      "/* 091 */\n",
      "/* 092 */     // common sub-expressions\n",
      "/* 093 */\n",
      "/* 094 */     // evaluate aggregate functions and update aggregation buffers\n",
      "/* 095 */\n",
      "/* 096 */     long agg_value_3 = agg_unsafeRowAggBuffer_0.getLong(0);\n",
      "/* 097 */\n",
      "/* 098 */     long agg_value_2 = -1L;\n",
      "/* 099 */\n",
      "/* 100 */     agg_value_2 = agg_value_3 + agg_expr_1_0;\n",
      "/* 101 */\n",
      "/* 102 */     agg_unsafeRowAggBuffer_0.setLong(0, agg_value_2);\n",
      "/* 103 */\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 107 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 108 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 109 */\n",
      "/* 110 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 111 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 112 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 113 */       long inputadapter_value_1 = inputadapter_row_0.getLong(1);\n",
      "/* 114 */\n",
      "/* 115 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1);\n",
      "/* 116 */       // shouldStop check is eliminated\n",
      "/* 117 */     }\n",
      "/* 118 */\n",
      "/* 119 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));\n",
      "/* 120 */   }\n",
      "/* 121 */\n",
      "/* 122 */   protected void processNext() throws java.io.IOException {\n",
      "/* 123 */     if (!agg_initAgg_0) {\n",
      "/* 124 */       agg_initAgg_0 = true;\n",
      "/* 125 */\n",
      "/* 126 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 127 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 128 */       agg_doAggregateWithKeys_0();\n",
      "/* 129 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 130 */     }\n",
      "/* 131 */     // output the result\n",
      "/* 132 */\n",
      "/* 133 */     while ( agg_mapIter_0.next()) {\n",
      "/* 134 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();\n",
      "/* 135 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();\n",
      "/* 136 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);\n",
      "/* 137 */       if (shouldStop()) return;\n",
      "/* 138 */     }\n",
      "/* 139 */     agg_mapIter_0.close();\n",
      "/* 140 */     if (agg_sorter_0 == null) {\n",
      "/* 141 */       agg_hashMap_0.free();\n",
      "/* 142 */     }\n",
      "/* 143 */   }\n",
      "/* 144 */\n",
      "/* 145 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",False)\n",
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().explain(\"codegen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d0c3359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* HashAggregate (6)\n",
      "+- Exchange (5)\n",
      "   +- * HashAggregate (4)\n",
      "      +- * Project (3)\n",
      "         +- * Filter (2)\n",
      "            +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [Card_type#17, Amount#21]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/C:/Users/nares/PySpark_Examples/customer.csv]\n",
      "PushedFilters: [IsNotNull(Amount)]\n",
      "ReadSchema: struct<Card_type:string,Amount:string>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [2]: [Card_type#17, Amount#21]\n",
      "Condition : (isnotnull(Amount#21) AND (cast(Amount#21 as int) >= 10))\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [1]: [Card_type#17]\n",
      "Input [2]: [Card_type#17, Amount#21]\n",
      "\n",
      "(4) HashAggregate [codegen id : 1]\n",
      "Input [1]: [Card_type#17]\n",
      "Keys [1]: [Card_Type#17]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#365L]\n",
      "Results [2]: [Card_Type#17, count#366L]\n",
      "\n",
      "(5) Exchange\n",
      "Input [2]: [Card_Type#17, count#366L]\n",
      "Arguments: hashpartitioning(Card_Type#17, 200), ENSURE_REQUIREMENTS, [plan_id=455]\n",
      "\n",
      "(6) HashAggregate [codegen id : 2]\n",
      "Input [2]: [Card_Type#17, count#366L]\n",
      "Keys [1]: [Card_Type#17]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#361L]\n",
      "Results [2]: [Card_Type#17, count(1)#361L AS count#362L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Amount >= 10\").groupBy(\"Card_Type\").count().explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471c874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
