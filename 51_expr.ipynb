{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6787cac6-c706-4245-ab74-967dc7d3c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Concatenate columns using || (sql like)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName('Examples.com').getOrCreate()\n",
    "\n",
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c111831a-1aa8-40fd-aeaf-e4bbcdf37675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name| gender|\n",
      "+-------+-------+\n",
      "|  James|   Male|\n",
      "|Michael| Female|\n",
      "|    Jen|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
    "columns = [\"name\",\"gender\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "#Using CASE WHEN similar to SQL.\n",
    "from pyspark.sql.functions import expr\n",
    "df2=df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f90ece-bd83-4e4c-ac84-523ce69d6bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "\n",
    "#Add Month value from another column\n",
    "df.select(df.date,df.increment,expr(\"add_months(date,increment)\").alias(\"inc_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40bf940d-96d4-4572-a1f7-a2dbfced0cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|      date|increment|increment_date|\n",
      "+----------+---------+--------------+\n",
      "|2019-01-23|        1|    2019-02-23|\n",
      "|2019-06-24|        2|    2019-08-24|\n",
      "|2019-09-20|        3|    2019-12-20|\n",
      "+----------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.select(df.date,df.increment,expr(\"\"\"add_months(date,increment) as increment_date\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0809249c-a235-45bb-8830-342092d1204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- increment: long (nullable = true)\n",
      " |-- str_increment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2bdc2e-f6ab-49e9-8cd0-497d983c6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            6|\n",
      "|2019-06-24|        2|            7|\n",
      "|2019-09-20|        3|            8|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.date,df.increment,expr(\"increment + 5 as new_increment\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1805020a-96e1-4ca6-8d23-d91959585444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "data=[(100,2),(200,3000),(500,500)] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.filter(expr(\"col1 == col2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a47261-85d2-475b-9b4a-29615dc0310d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
