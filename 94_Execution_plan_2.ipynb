{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d5490e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-------+------+--------+\n",
      "|emp_id|  name| doj|dept_id|gender|  salary|\n",
      "+------+------+----+-------+------+--------+\n",
      "|    10|  Nani|2000|    100|     M| 2500000|\n",
      "|    20| Nikky|2022|    200|     M|50000000|\n",
      "|    30|Raghav|2010|   null|     F|   20000|\n",
      "|    40|  Raja|2012|    100|  null|  450000|\n",
      "|    50|  Rama|2014|    400|     F|  442000|\n",
      "|    60| Rasul|2015|    500|     M|  562000|\n",
      "+------+------+----+-------+------+--------+\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|     Nani|    100|\n",
      "|     Janu|    200|\n",
      "|    Nikky|   null|\n",
      "|    Lukky|    300|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('app').getOrCreate()\n",
    "\n",
    "emp_data = [(10,\"Nani\",\"2000\",\"100\",\"M\",2500000),\n",
    "            (20,\"Nikky\",\"2022\",\"200\",\"M\",50000000),\n",
    "            (30,\"Raghav\",\"2010\",None,\"F\",20000),\n",
    "            (40,\"Raja\",\"2012\",\"100\",None,450000),\n",
    "            (50,\"Rama\",\"2014\",\"400\",\"F\",442000),\n",
    "            (60,\"Rasul\",\"2015\",\"500\",\"M\",562000)]\n",
    "\n",
    "emp_schema = [\"emp_id\",\"name\",\"doj\",\"dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "emp_df.show()\n",
    "\n",
    "dept_data =[(\"Nani\",100),\n",
    "            (\"Janu\",200),\n",
    "            (\"Nikky\",None),\n",
    "            (\"Lukky\",300)]\n",
    "\n",
    "dept_schema = [\"dept_name\",\"dept_id\"]\n",
    "dept_df = spark.createDataFrame(data=dept_data,schema=dept_schema)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b3974",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32eba6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|dept_name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|     Janu|   50000000|\n",
      "|     Nani|    2950000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfjoin = emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,\"inner\")\\\n",
    "               .withColumn(\"bonus\",col(\"salary\")*0.1) \\\n",
    "               .groupBy(\"dept_name\").sum(\"salary\")\n",
    "dfjoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d48d28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept_name#532], functions=[sum(salary#500L)])\n",
      "   +- Exchange hashpartitioning(dept_name#532, 200), ENSURE_REQUIREMENTS, [plan_id=955]\n",
      "      +- HashAggregate(keys=[dept_name#532], functions=[partial_sum(salary#500L)])\n",
      "         +- Project [salary#500L, dept_name#532]\n",
      "            +- SortMergeJoin [cast(dept_id#498 as bigint)], [dept_id#533L], Inner\n",
      "               :- Sort [cast(dept_id#498 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(cast(dept_id#498 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=947]\n",
      "               :     +- Project [dept_id#498, salary#500L]\n",
      "               :        +- Filter isnotnull(dept_id#498)\n",
      "               :           +- Scan ExistingRDD[emp_id#495L,name#496,doj#497,dept_id#498,gender#499,salary#500L]\n",
      "               +- Sort [dept_id#533L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(dept_id#533L, 200), ENSURE_REQUIREMENTS, [plan_id=948]\n",
      "                     +- Filter isnotnull(dept_id#533L)\n",
      "                        +- Scan ExistingRDD[dept_name#532,dept_id#533L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5629c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept_name#202], functions=[sum(salary#170L)])\n",
      "   +- Exchange hashpartitioning(dept_name#202, 200), ENSURE_REQUIREMENTS, [plan_id=341]\n",
      "      +- HashAggregate(keys=[dept_name#202], functions=[partial_sum(salary#170L)])\n",
      "         +- Project [salary#170L, dept_name#202]\n",
      "            +- SortMergeJoin [cast(dept_id#168 as bigint)], [dept_id#203L], Inner\n",
      "               :- Sort [cast(dept_id#168 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(cast(dept_id#168 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=333]\n",
      "               :     +- Project [dept_id#168, salary#170L]\n",
      "               :        +- Filter isnotnull(dept_id#168)\n",
      "               :           +- Scan ExistingRDD[emp_id#165L,name#166,doj#167,dept_id#168,gender#169,salary#170L]\n",
      "               +- Sort [dept_id#203L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(dept_id#203L, 200), ENSURE_REQUIREMENTS, [plan_id=334]\n",
      "                     +- Filter isnotnull(dept_id#203L)\n",
      "                        +- Scan ExistingRDD[dept_name#202,dept_id#203L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9550a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['dept_name], ['dept_name, sum(salary#170L) AS sum(salary)#251L]\n",
      "+- Project [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L, dept_name#202, dept_id#203L, (cast(salary#170L as double) * 0.1) AS bonus#231]\n",
      "   +- Join Inner, (cast(dept_id#168 as bigint) = dept_id#203L)\n",
      "      :- LogicalRDD [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L], false\n",
      "      +- LogicalRDD [dept_name#202, dept_id#203L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "dept_name: string, sum(salary): bigint\n",
      "Aggregate [dept_name#202], [dept_name#202, sum(salary#170L) AS sum(salary)#251L]\n",
      "+- Project [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L, dept_name#202, dept_id#203L, (cast(salary#170L as double) * 0.1) AS bonus#231]\n",
      "   +- Join Inner, (cast(dept_id#168 as bigint) = dept_id#203L)\n",
      "      :- LogicalRDD [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L], false\n",
      "      +- LogicalRDD [dept_name#202, dept_id#203L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [dept_name#202], [dept_name#202, sum(salary#170L) AS sum(salary)#251L]\n",
      "+- Project [salary#170L, dept_name#202]\n",
      "   +- Join Inner, (cast(dept_id#168 as bigint) = dept_id#203L)\n",
      "      :- Project [dept_id#168, salary#170L]\n",
      "      :  +- Filter isnotnull(dept_id#168)\n",
      "      :     +- LogicalRDD [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L], false\n",
      "      +- Filter isnotnull(dept_id#203L)\n",
      "         +- LogicalRDD [dept_name#202, dept_id#203L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept_name#202], functions=[sum(salary#170L)], output=[dept_name#202, sum(salary)#251L])\n",
      "   +- Exchange hashpartitioning(dept_name#202, 200), ENSURE_REQUIREMENTS, [plan_id=341]\n",
      "      +- HashAggregate(keys=[dept_name#202], functions=[partial_sum(salary#170L)], output=[dept_name#202, sum#261L])\n",
      "         +- Project [salary#170L, dept_name#202]\n",
      "            +- SortMergeJoin [cast(dept_id#168 as bigint)], [dept_id#203L], Inner\n",
      "               :- Sort [cast(dept_id#168 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(cast(dept_id#168 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=333]\n",
      "               :     +- Project [dept_id#168, salary#170L]\n",
      "               :        +- Filter isnotnull(dept_id#168)\n",
      "               :           +- Scan ExistingRDD[emp_id#165L,name#166,doj#167,dept_id#168,gender#169,salary#170L]\n",
      "               +- Sort [dept_id#203L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(dept_id#203L, 200), ENSURE_REQUIREMENTS, [plan_id=334]\n",
      "                     +- Filter isnotnull(dept_id#203L)\n",
      "                        +- Scan ExistingRDD[dept_name#202,dept_id#203L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e77350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (15)\n",
      "+- HashAggregate (14)\n",
      "   +- Exchange (13)\n",
      "      +- HashAggregate (12)\n",
      "         +- Project (11)\n",
      "            +- SortMergeJoin Inner (10)\n",
      "               :- Sort (5)\n",
      "               :  +- Exchange (4)\n",
      "               :     +- Project (3)\n",
      "               :        +- Filter (2)\n",
      "               :           +- Scan ExistingRDD (1)\n",
      "               +- Sort (9)\n",
      "                  +- Exchange (8)\n",
      "                     +- Filter (7)\n",
      "                        +- Scan ExistingRDD (6)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD\n",
      "Output [6]: [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L]\n",
      "Arguments: [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L], MapPartitionsRDD[44] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter\n",
      "Input [6]: [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L]\n",
      "Condition : isnotnull(dept_id#168)\n",
      "\n",
      "(3) Project\n",
      "Output [2]: [dept_id#168, salary#170L]\n",
      "Input [6]: [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L]\n",
      "\n",
      "(4) Exchange\n",
      "Input [2]: [dept_id#168, salary#170L]\n",
      "Arguments: hashpartitioning(cast(dept_id#168 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=333]\n",
      "\n",
      "(5) Sort\n",
      "Input [2]: [dept_id#168, salary#170L]\n",
      "Arguments: [cast(dept_id#168 as bigint) ASC NULLS FIRST], false, 0\n",
      "\n",
      "(6) Scan ExistingRDD\n",
      "Output [2]: [dept_name#202, dept_id#203L]\n",
      "Arguments: [dept_name#202, dept_id#203L], MapPartitionsRDD[51] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(7) Filter\n",
      "Input [2]: [dept_name#202, dept_id#203L]\n",
      "Condition : isnotnull(dept_id#203L)\n",
      "\n",
      "(8) Exchange\n",
      "Input [2]: [dept_name#202, dept_id#203L]\n",
      "Arguments: hashpartitioning(dept_id#203L, 200), ENSURE_REQUIREMENTS, [plan_id=334]\n",
      "\n",
      "(9) Sort\n",
      "Input [2]: [dept_name#202, dept_id#203L]\n",
      "Arguments: [dept_id#203L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(10) SortMergeJoin\n",
      "Left keys [1]: [cast(dept_id#168 as bigint)]\n",
      "Right keys [1]: [dept_id#203L]\n",
      "Join condition: None\n",
      "\n",
      "(11) Project\n",
      "Output [2]: [salary#170L, dept_name#202]\n",
      "Input [4]: [dept_id#168, salary#170L, dept_name#202, dept_id#203L]\n",
      "\n",
      "(12) HashAggregate\n",
      "Input [2]: [salary#170L, dept_name#202]\n",
      "Keys [1]: [dept_name#202]\n",
      "Functions [1]: [partial_sum(salary#170L)]\n",
      "Aggregate Attributes [1]: [sum#260L]\n",
      "Results [2]: [dept_name#202, sum#261L]\n",
      "\n",
      "(13) Exchange\n",
      "Input [2]: [dept_name#202, sum#261L]\n",
      "Arguments: hashpartitioning(dept_name#202, 200), ENSURE_REQUIREMENTS, [plan_id=341]\n",
      "\n",
      "(14) HashAggregate\n",
      "Input [2]: [dept_name#202, sum#261L]\n",
      "Keys [1]: [dept_name#202]\n",
      "Functions [1]: [sum(salary#170L)]\n",
      "Aggregate Attributes [1]: [sum(salary#170L)#250L]\n",
      "Results [2]: [dept_name#202, sum(salary#170L)#250L AS sum(salary)#251L]\n",
      "\n",
      "(15) AdaptiveSparkPlan\n",
      "Output [2]: [dept_name#202, sum(salary)#251L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8452c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Aggregate [dept_name#202], [dept_name#202, sum(salary#170L) AS sum(salary)#251L], Statistics(sizeInBytes=1.66E+37 B)\n",
      "+- Project [salary#170L, dept_name#202], Statistics(sizeInBytes=1.66E+37 B)\n",
      "   +- Join Inner, (cast(dept_id#168 as bigint) = dept_id#203L), Statistics(sizeInBytes=2.94E+37 B)\n",
      "      :- Project [dept_id#168, salary#170L], Statistics(sizeInBytes=2.8 EiB)\n",
      "      :  +- Filter isnotnull(dept_id#168), Statistics(sizeInBytes=8.0 EiB)\n",
      "      :     +- LogicalRDD [emp_id#165L, name#166, doj#167, dept_id#168, gender#169, salary#170L], false, Statistics(sizeInBytes=8.0 EiB)\n",
      "      +- Filter isnotnull(dept_id#203L), Statistics(sizeInBytes=8.0 EiB)\n",
      "         +- LogicalRDD [dept_name#202, dept_id#203L], false, Statistics(sizeInBytes=8.0 EiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[dept_name#202], functions=[sum(salary#170L)], output=[dept_name#202, sum(salary)#251L])\n",
      "   +- Exchange hashpartitioning(dept_name#202, 200), ENSURE_REQUIREMENTS, [plan_id=341]\n",
      "      +- HashAggregate(keys=[dept_name#202], functions=[partial_sum(salary#170L)], output=[dept_name#202, sum#261L])\n",
      "         +- Project [salary#170L, dept_name#202]\n",
      "            +- SortMergeJoin [cast(dept_id#168 as bigint)], [dept_id#203L], Inner\n",
      "               :- Sort [cast(dept_id#168 as bigint) ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(cast(dept_id#168 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=333]\n",
      "               :     +- Project [dept_id#168, salary#170L]\n",
      "               :        +- Filter isnotnull(dept_id#168)\n",
      "               :           +- Scan ExistingRDD[emp_id#165L,name#166,doj#167,dept_id#168,gender#169,salary#170L]\n",
      "               +- Sort [dept_id#203L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(dept_id#203L, 200), ENSURE_REQUIREMENTS, [plan_id=334]\n",
      "                     +- Filter isnotnull(dept_id#203L)\n",
      "                        +- Scan ExistingRDD[dept_name#202,dept_id#203L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5b4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
