{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7daf586-c4f8-43fe-b11e-4bea11cbd75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|  Alice| 25|\n",
      "|Charlie| 22|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|  Alice| 25|\n",
      "|Charlie| 22|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|Name   |Age|\n",
      "+-------+---+\n",
      "|Alice  |25 |\n",
      "|Bob    |30 |\n",
      "|Charlie|22 |\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "data1 = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "data2 = [(\"Alice\", 25), (\"Charlie\", 22)]\n",
    "df1 = spark.createDataFrame(data1, [\"Name\", \"Age\"])\n",
    "df2 = spark.createDataFrame(data2, [\"Name\", \"Age\"])\n",
    "\n",
    "unioned_df = df1.union(df2)\n",
    "unioned_df.show()\n",
    "\n",
    "unioned_df1 = df1.unionAll(df2)\n",
    "unioned_df1.show()\n",
    "\n",
    "disDF = df1.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e476a3-8901-45bf-9635-7fe9da88ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  James| 34|\n",
      "|Michael| 56|\n",
      "| Robert| 30|\n",
      "|  Maria| 24|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "| 34|James|\n",
      "| 45|Maria|\n",
      "| 45|  Jen|\n",
      "| 34| Jeff|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#unionByName\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('com').getOrCreate()\n",
    "\n",
    "data = [(\"James\",34), (\"Michael\",56),(\"Robert\",30), (\"Maria\",24) ]\n",
    "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "\n",
    "data2=[(34,\"James\"),(45,\"Maria\"),(45,\"Jen\"),(34,\"Jeff\")]\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841ff2b3-b761-43d5-ae25-d176295227bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  James| 34|\n",
      "|Michael| 56|\n",
      "| Robert| 30|\n",
      "|  Maria| 24|\n",
      "|  James| 34|\n",
      "|  Maria| 45|\n",
      "|    Jen| 45|\n",
      "|   Jeff| 34|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.unionByName(df2)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3db023b-ce74-4430-8b7b-11e5c2441a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col0|col1|col2|col3|\n",
      "+----+----+----+----+\n",
      "|   5|   2|   6|null|\n",
      "|null|   6|   7|   3|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Using allowMissingColumns\n",
    "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df3.printSchema\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13a546-aa99-4beb-ae3e-dc400a691c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
